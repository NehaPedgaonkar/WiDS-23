{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ab37f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MDP:\n",
    "    def __init__(self, S, A, gamma, mdptype, endstates, transitionprob, rewards):\n",
    "        self.S = S #number of states\n",
    "        self.A = A #number of actions\n",
    "        self.gamma = gamma #gamma is the discount factor, is a value between 0 and 1 that represents the preference for current rewards over future rewards in the decision-making process.\n",
    "        self.mdptype = mdptype\n",
    "        self.endstates = endstates\n",
    "        self.transitionprob = transitionprob #transition probability\n",
    "        self.rewards = rewards\n",
    "\n",
    "        if mdptype == \"continuing\":\n",
    "            self.continuing_mdp_solving()\n",
    "        else:\n",
    "            self.episodic_mdp_solving()\n",
    "\n",
    "    def continuing_mdp_solving(self):\n",
    "        self.valuef = np.zeros(self.S) # valuef = value function for each state in the MDP. \n",
    "        self.ep = 1e-9 #ep=epsilon, a small positive value used as a threshold for convergence in the value iteration loop, determines the stopping criteria for the iterative process.\n",
    "        self.optimact = np.zeros(self.S, dtype=int) #optimact=optimal action or policy, here is used as an array to store the policy for each state, represents the optimal action to take in each state. Here, optimact is an array where optimact[s] stores the optimal action to take in state s according to the current policy.\n",
    "\n",
    "        self.flag = 0  # Initialize the flag variable\n",
    "\n",
    "        while self.flag == 0:  # Use the flag in the loop condition\n",
    "            \n",
    "            self.diff = 0 # diff represents the maximum change in the values of the states during the value iteration process.\n",
    "            self.valuef1 = self.valuef.copy()\n",
    "            self.optimact1 = self.optimact.copy()\n",
    "            for self.s in range(self.S):\n",
    "                self.actions = []\n",
    "                for self.a in range(self.A):\n",
    "                    self.summation = 0\n",
    "                    for self._s in range(self.S):\n",
    "                        if self.transitionprob[self.s][self.a][self._s] != 0:\n",
    "                            self.summation += (\n",
    "                                self.transitionprob[self.s][self.a][self._s]\n",
    "                                * (self.rewards[self.s][self.a][self._s] + self.gamma * self.valuef[self._s])\n",
    "                            )\n",
    "                    self.actions.append(self.summation)\n",
    "                self.valuef1[self.s] = max(self.actions)\n",
    "                self.optimact1[self.s] = np.argmax(self.actions)\n",
    "\n",
    "                self.diff = max(self.diff, abs(self.valuef[self.s] - self.valuef1[self.s]))\n",
    "\n",
    "            self.valuef = self.valuef1\n",
    "            self.optimact = self.optimact1\n",
    "\n",
    "            if self.diff < self.ep:\n",
    "                \n",
    "                self.flag = 1  # Set the flag to exit the loop\n",
    "\n",
    "        output_file = f\"sol-{self.mdptype}-mdp-{self.S}-{self.A}.txt\"\n",
    "        with open(output_file, 'w') as outfile:\n",
    "            for i in range(self.S):\n",
    "                outfile.write(f\"{np.round(self.valuef[i], 6)} {self.optimact[i]}\\n\")\n",
    "\n",
    "    def episodic_mdp_solving(self):\n",
    "        self.valuef = np.zeros(self.S)\n",
    "        self.ep = 1e-9\n",
    "        self.optimact = np.zeros(self.S, dtype=int)\n",
    "\n",
    "        self.flag = 0  # Initialize the flag variable\n",
    "\n",
    "        while self.flag == 0:  # Use the flag in the loop condition\n",
    "            \n",
    "            self.diff = 0\n",
    "            self.valuef1 = self.valuef.copy()\n",
    "            self.optimact1 = self.optimact.copy()\n",
    "            for self.s in range(self.S):\n",
    "\n",
    "                self.actions = []\n",
    "                if self.s in self.endstates:\n",
    "                    self.valuef1[self.s] = 0\n",
    "                    self.optimact1[self.s] = 0\n",
    "                else:\n",
    "                    for self.a in range(self.A):\n",
    "                        self.summation = 0\n",
    "\n",
    "                        for self._s in range(self.S):\n",
    "                            if self.transitionprob[self.s][self.a][self._s] != 0:\n",
    "                                self.summation += (\n",
    "                                    self.transitionprob[self.s][self.a][self._s]\n",
    "                                    * (self.rewards[self.s][self.a][self._s] + self.gamma * self.valuef[self._s])\n",
    "                                )\n",
    "                        self.actions.append(self.summation)\n",
    "                    self.valuef1[self.s] = max(self.actions)\n",
    "                    self.optimact1[self.s] = np.argmax(self.actions)\n",
    "\n",
    "                    self.diff = max(self.diff, abs(self.valuef[self.s] - self.valuef1[self.s]))\n",
    "\n",
    "            if self.diff < self.ep:\n",
    "                \n",
    "                self.flag = 1  # Set the flag to exit the loop\n",
    "\n",
    "            self.valuef = self.valuef1\n",
    "            self.optimact = self.optimact1\n",
    "\n",
    "        output_file = f\"sol-{self.mdptype}-mdp-{self.S}-{self.A}.txt\"\n",
    "        with open(output_file, 'w') as outfile: # write mode\n",
    "            for i in range(self.S):\n",
    "                outfile.write(f\"{np.round(self.valuef[i], 6)} {self.optimact[i]}\\n\")\n",
    "\n",
    "    def read_mdp_file(self, file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        return lines\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_files = [\n",
    "        \"/Users/nehavpedgaonkar/Downloads/data 2/continuing-mdp-2-2.txt\",\n",
    "        \"/Users/nehavpedgaonkar/Downloads/data 2/continuing-mdp-10-5.txt\",\n",
    "        \"/Users/nehavpedgaonkar/Downloads/data 2/continuing-mdp-50-20.txt\",\n",
    "        \"/Users/nehavpedgaonkar/Downloads/data 2/episodic-mdp-2-2.txt\",\n",
    "        \"/Users/nehavpedgaonkar/Downloads/data 2/episodic-mdp-10-5.txt\",\n",
    "        \"/Users/nehavpedgaonkar/Downloads/data 2/episodic-mdp-50-20.txt\"\n",
    "    ]\n",
    "    for input_file in input_files:\n",
    "        with open(input_file, 'r') as file: #read mode\n",
    "            lines = file.readlines()\n",
    "\n",
    "        S = int(lines[0].split()[1])\n",
    "        A = int(lines[1].split()[1])\n",
    "        endstates = lines[2].split()[1:]\n",
    "        transitionprob = np.zeros((S, A, S))\n",
    "        rewards = np.zeros((S, A, S))\n",
    "        for line in lines[3:-2]:\n",
    "            transitionprob[int(line.split()[1])][int(line.split()[2])][int(line.split()[3])] = float(line.split()[5])\n",
    "            rewards[int(line.split()[1])][int(line.split()[2])][int(line.split()[3])] = float(line.split()[4]) \n",
    "\n",
    "        mdptype = lines[-2].split()[1]\n",
    "        gamma = float(lines[-1].split()[1])\n",
    "\n",
    "        mdp = MDP(S, A, gamma, mdptype, endstates, transitionprob, rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169a59f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
